{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPeZaA0NUkjuVO7yby87g2G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import librosa\n","import torch\n","from tqdm import tqdm\n","import numpy as np\n","from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model"],"metadata":{"id":"ZbaSGqT6PzvE","executionInfo":{"status":"ok","timestamp":1713189772491,"user_tz":-120,"elapsed":305,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","directory_path = '/content/drive/My Drive/NLP/clips__test'\n","\n","print(len(os.listdir(directory_path)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umZPI89LQB1y","executionInfo":{"status":"ok","timestamp":1713189777063,"user_tz":-120,"elapsed":1988,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}},"outputId":"f6af4b48-0ede-49af-8d5b-17294dcb8702"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","5\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"L15GvisEPtQl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713187752010,"user_tz":-120,"elapsed":769340,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}},"outputId":"8357ce04-67eb-49e4-af96-e6c6a1a8fda2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 364/364 [01:14<00:00,  4.89it/s]\n","100%|██████████| 362/362 [01:04<00:00,  5.59it/s]\n","100%|██████████| 358/358 [01:12<00:00,  4.97it/s]\n","100%|██████████| 346/346 [01:06<00:00,  5.17it/s]\n","100%|██████████| 356/356 [01:21<00:00,  4.36it/s]\n","100%|██████████| 367/367 [01:14<00:00,  4.95it/s]\n","100%|██████████| 351/351 [01:15<00:00,  4.62it/s]\n","100%|██████████| 366/366 [01:08<00:00,  5.37it/s]\n","100%|██████████| 354/354 [01:09<00:00,  5.09it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","100%|██████████| 62/62 [00:44<00:00,  1.39it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","100%|██████████| 364/364 [01:13<00:00,  4.94it/s]\n"]}],"source":["# using GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", output_hidden_states=True)\n","model.to(device)  # Using identified device\n","\n","def load_audio_files(directory, layer_indices=[-1]):\n","    \"\"\"Lädt alle MP3-Dateien im angegebenen Verzeichnis und extrahiert die Repräsentationen aus den spezifizierten Schichten.\"\"\"\n","    for filename in tqdm(os.listdir(directory)):\n","        if filename.endswith(\".mp3\"):\n","            file_path = os.path.join(directory, filename)\n","            audio, sr = librosa.load(file_path, sr=16000)\n","            input_values = feature_extractor(audio, return_tensors=\"pt\", sampling_rate=sr).input_values\n","            input_values = input_values.to(device)\n","            with torch.no_grad():\n","                outputs = model(input_values)\n","                for index in layer_indices:\n","                    hidden_states = outputs.hidden_states[index]\n","                    # creating sub directory for each layer in speaker directory\n","                    layer_dir = os.path.join(directory, f\"layer_{index}\")\n","                    os.makedirs(layer_dir, exist_ok=True)\n","                    save_path = os.path.join(layer_dir, f\"{os.path.splitext(filename)[0]}_layer_{index}.npy\")\n","                    np.save(save_path, hidden_states.cpu().numpy())\n","                    # print(f\"Processed and saved: {filename} in {save_path}\")\n","\n","\n","\n","for d in os.listdir(directory_path):\n","  dir = os.path.join(directory_path, d)\n","  load_audio_files(dir, layer_indices=[0, 5, 10, 15, 20, 24])"]}]}