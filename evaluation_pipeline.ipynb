{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyPLrkSzLiUhwxjzsJtAmz4C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Installing packages"],"metadata":{"id":"ALDFkYLBLMHA"}},{"cell_type":"code","source":["!pip install datasets --quiet\n","!pip install torch --quiet\n","!pip install keras --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfbIETa2LQN9","executionInfo":{"status":"ok","timestamp":1714051020721,"user_tz":-120,"elapsed":72874,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}},"outputId":"1afd4611-6d51-465f-d3e1-cdc09b1681f6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["Importing relevant packages"],"metadata":{"id":"BMKuW4YBLrh2"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"0dzMVDvJK9zX","executionInfo":{"status":"ok","timestamp":1714051028226,"user_tz":-120,"elapsed":7509,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import pandas as pd\n","from tqdm import tqdm"]},{"cell_type":"markdown","source":["Load data for training"],"metadata":{"id":"9d9OQr8mMW4r"}},{"cell_type":"code","source":["np.random.seed(42)  # Set the seed for NumPy random number generation\n","\n","def load_data(base_dir, layer):\n","    embeddings = []\n","    labels = []\n","    label_map = {}\n","    current_label = 0\n","\n","    # Iterate over each speaker's directory\n","    for speaker_dir in os.listdir(base_dir):\n","        # Build the path to the specific layer for the current speaker\n","        layer_dir = os.path.join(base_dir, speaker_dir, layer)\n","\n","        if os.path.isdir(layer_dir):\n","            # Load all .npy files in this layer directory\n","            for file_name in os.listdir(layer_dir):\n","                if file_name.endswith('.npy'):\n","                    path = os.path.join(layer_dir, file_name)\n","                    embedding = np.load(path)\n","                    embeddings.append(embedding)\n","\n","                    # Map speaker to a label if not already done\n","                    if speaker_dir not in label_map:\n","                        label_map[speaker_dir] = current_label\n","                        current_label += 1\n","\n","                    # Append the label for each embedding\n","                    labels.append(label_map[speaker_dir])\n","\n","    # Convert list of embeddings and labels to numpy arrays\n","    embeddings = np.array(embeddings)\n","    labels = np.array(labels)\n","    return embeddings, labels\n"],"metadata":{"id":"lbODOJ8GMEQA","executionInfo":{"status":"ok","timestamp":1714051028226,"user_tz":-120,"elapsed":6,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Creating model"],"metadata":{"id":"-u2FlVSuQa81"}},{"cell_type":"code","source":["class CNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=(5, 5), padding=(2, 2))\n","        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1))\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1, 1))\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(128 * 12 * 128, 128)  # Adjust the flattened size according to your input shape\n","        self.fc2 = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv3(x))\n","        x = self.pool(x)\n","        x = x.view(-1, 128 * 12 * 128)  # Flatten the tensor for the fully connected layer\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n"],"metadata":{"id":"jNt0c7csQZIP","executionInfo":{"status":"ok","timestamp":1714051028226,"user_tz":-120,"elapsed":5,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Compiling the model"],"metadata":{"id":"e6XlQzNIQ74j"}},{"cell_type":"code","source":["model = CNN(num_classes=10)  # Change num_classes as per your dataset\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"jC00_gXpQ6UP","executionInfo":{"status":"ok","timestamp":1714051030220,"user_tz":-120,"elapsed":1999,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Function to evaluate model"],"metadata":{"id":"0PmlLPXwRVA8"}},{"cell_type":"code","source":["def evaluate_model(model, loader, device):\n","    y_true = []\n","    y_pred = []\n","    model.eval()\n","    with torch.no_grad():\n","        for inputs, targets in loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            y_pred.extend(predicted.cpu().numpy())\n","            y_true.extend(targets.cpu().numpy())\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n","    return accuracy, precision, recall, f1\n"],"metadata":{"id":"oNXFWZC0SwT2","executionInfo":{"status":"ok","timestamp":1714051030220,"user_tz":-120,"elapsed":7,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Function to train und evaluate all layers"],"metadata":{"id":"JQrARRdpZusx"}},{"cell_type":"code","source":["def train_and_evaluate(base_dir, layers, num_classes, device, epochs=30):\n","    results = []\n","    for layer in layers:\n","        embeddings, labels = load_data(base_dir, layer)\n","        dataset = TensorDataset(torch.from_numpy(embeddings), torch.from_numpy(labels))\n","        train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","        model = CNN(num_classes=num_classes).to(device)\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        epoch_data = {'epoch': [], 'train_loss': [], 'val_accuracy': [], 'val_precision': [], 'val_recall': [], 'val_f1': []}\n","\n","        for epoch in range(epochs):  # Adjust epochs if necessary\n","            model.train()\n","            total_loss = 0\n","            with tqdm(train_loader, desc=f\"Layer: {layer}, Epoch: {epoch+1}\", unit=\"batch\") as t:\n","                for inputs, targets in t:\n","                    inputs, targets = inputs.to(device), targets.to(device)\n","                    optimizer.zero_grad()\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, targets)\n","                    loss.backward()\n","                    optimizer.step()\n","                    total_loss += loss.item()\n","                    t.set_postfix(loss=loss.item())\n","\n","            # Evaluation on the validation set\n","            accuracy, precision, recall, f1 = evaluate_model(model, train_loader, device)\n","\n","            # Store metrics for each epoch\n","            epoch_data['epoch'].append(epoch)\n","            epoch_data['train_loss'].append(total_loss / len(train_loader))\n","            epoch_data['val_accuracy'].append(accuracy)\n","            epoch_data['val_precision'].append(precision)\n","            epoch_data['val_recall'].append(recall)\n","            epoch_data['val_f1'].append(f1)\n","\n","        # Store final results\n","        results.append((layer, accuracy, precision, recall, f1))\n","        # Convert epoch data to DataFrame and save to CSV\n","        df = pd.DataFrame(epoch_data)\n","        df.to_csv(f\"{layer}_training_progress.csv\", index=False)\n","\n","    return results"],"metadata":{"id":"qJnbXmf5VLTz","executionInfo":{"status":"ok","timestamp":1714051281642,"user_tz":-120,"elapsed":358,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Save results as CSV"],"metadata":{"id":"GcTa5TYoZ5F0"}},{"cell_type":"code","source":["def save_final_results(results):\n","    df_results = pd.DataFrame(results, columns=['Layer', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n","    df_results.to_csv(\"final_results.csv\", index=False)"],"metadata":{"id":"ieqkuLbPVP9r","executionInfo":{"status":"ok","timestamp":1714051284420,"user_tz":-120,"elapsed":387,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Conducting training and evaluation"],"metadata":{"id":"RhC2J_uhZ8_i"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","base_dir = '/content/drive/My Drive/new_speaker_identification/clips__test/'\n","# layers = ['layer_0_processed', 'layer_5_processed', 'layer_10_processed', 'layer_20_processed', 'layer_24_processed']\n","layers = ['layer_0_processed', 'layer_5_processed']\n","num_classes = 25\n","results = train_and_evaluate(base_dir, layers, num_classes, device)\n","save_final_results(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1YY5IIRRUc24","outputId":"621390ed-d0a8-48df-d776-d64f8a073061"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}]}